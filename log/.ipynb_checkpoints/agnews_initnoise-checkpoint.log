nohup: ignoring input

===================================BUG REPORT===================================
Welcome to bitsandbytes. For bug reports, please run

python -m bitsandbytes

 and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues
================================================================================
bin /home/baishengyuan/anaconda3/envs/nllm/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda118.so
CUDA SETUP: CUDA runtime path found: /home/baishengyuan/anaconda3/envs/nllm/lib/libcudart.so.11.0
CUDA SETUP: Highest compute capability among GPUs detected: 8.6
CUDA SETUP: Detected CUDA version 118
CUDA SETUP: Loading binary /home/baishengyuan/anaconda3/envs/nllm/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda118.so...
[2024-01-12 17:31:36,762] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)
2024-01-12 17:31:41.943 | INFO     | __main__:init_components:107 - Initializing components...
The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
Loading checkpoint shards:   0%|                                                                                                                                                             | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|██████████████████████████████████████████████████████████████████████████▌                                                                          | 1/2 [00:07<00:07,  7.32s/it]Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:09<00:00,  4.55s/it]Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:09<00:00,  4.96s/it]
You are using the legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This means that tokens that come after special tokens will not be properly handled. We recommend you to read the related pull request available at https://github.com/huggingface/transformers/pull/24565
2024-01-12 17:32:00.372 | INFO     | utils.dataset_new:__init__:189 - Loading data: /cto_labs/baishengyuan/noise_llm_data/tex_class/ag_news
2024-01-12 17:32:02.830 | INFO     | utils.dataset_new:__init__:198 - there are 120000 data in dataset
2024-01-12 17:32:02.848 | INFO     | utils.dataset_new:__init__:189 - Loading data: /cto_labs/baishengyuan/noise_llm_data/tex_class/ag_news
2024-01-12 17:32:02.990 | INFO     | utils.dataset_new:__init__:198 - there are 7600 data in dataset
