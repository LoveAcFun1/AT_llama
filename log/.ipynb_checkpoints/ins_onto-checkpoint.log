nohup: ignoring input

===================================BUG REPORT===================================
Welcome to bitsandbytes. For bug reports, please run

python -m bitsandbytes

 and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues
================================================================================
bin /home/baishengyuan/anaconda3/envs/nllm/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda118.so
CUDA SETUP: CUDA runtime path found: /home/baishengyuan/anaconda3/envs/nllm/lib/libcudart.so.11.0
CUDA SETUP: Highest compute capability among GPUs detected: 8.6
CUDA SETUP: Detected CUDA version 118
CUDA SETUP: Loading binary /home/baishengyuan/anaconda3/envs/nllm/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda118.so...
[2024-01-19 19:10:36,960] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)
2024-01-19 19:10:42.026 | INFO     | __main__:init_components:107 - Initializing components...
The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
Loading checkpoint shards:   0%|                                                                                                                                                                 | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|████████████████████████████████████████████████████████████████████████████▌                                                                            | 1/2 [00:07<00:07,  7.33s/it]Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:09<00:00,  4.56s/it]Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:09<00:00,  4.98s/it]
You are using the legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This means that tokens that come after special tokens will not be properly handled. We recommend you to read the related pull request available at https://github.com/huggingface/transformers/pull/24565
2024-01-19 19:11:01.849 | INFO     | utils.dataset_new:__init__:189 - Loading data: /cto_labs/baishengyuan/noise_llm_data/NER/Ontonotes
2024-01-19 19:11:02.486 | INFO     | utils.dataset_new:__init__:198 - there are 59924 data in dataset
2024-01-19 19:11:02.502 | INFO     | utils.dataset_new:__init__:189 - Loading data: /cto_labs/baishengyuan/noise_llm_data/NER/Ontonotes
2024-01-19 19:11:02.575 | INFO     | utils.dataset_new:__init__:198 - there are 8262 data in dataset
